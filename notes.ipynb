{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing out if I can turn the grid into a vector transport system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy matplotlib scipy sympy tensorflow keras scikit-learn pandas pillow h5py pydot graphviz opencv-python flask gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GridAnalyzer:\n",
    "def **init**(self, filepath):\n",
    "self.grid = pd.read_excel(filepath, header=None).values\n",
    "if self.grid.shape != (100, 100):\n",
    "raise ValueError(\"Grid must be 100x100.\")\n",
    "\n",
    "    def get_value(self, x, y):\n",
    "        return self.grid[x, y]\n",
    "\n",
    "    def first_row_values(self, length=10):\n",
    "        return self.grid[0, :length]\n",
    "\n",
    "    def first_column_values(self, length=10):\n",
    "        return self.grid[:length, 0]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_fibonacci(sequence):\n",
    "        fib_seq = [0, 1]\n",
    "        while fib_seq[-1] + fib_seq[-2] <= sequence[-1]:\n",
    "            fib_seq.append(fib_seq[-1] + fib_seq[-2])\n",
    "        return sequence == fib_seq[len(fib_seq)-len(sequence):]\n",
    "\n",
    "    def apply_rule(self, rule_func):\n",
    "        transformed_grid = np.zeros_like(self.grid, dtype=float)\n",
    "        for x in range(self.grid.shape[0]):\n",
    "            for y in range(self.grid.shape[1]):\n",
    "                transformed_grid[x, y] = rule_func(self, x, y)\n",
    "        return transformed_grid\n",
    "\n",
    "def curry_rule(base_rule):\n",
    "def curried_rule(x, y):\n",
    "def apply_rule(grid_analyzer):\n",
    "return base_rule(grid_analyzer, x, y)\n",
    "return apply_rule\n",
    "return curried_rule\n",
    "\n",
    "# Example of a base rule function\n",
    "\n",
    "def example_rule(analyzer, x, y): # This is a placeholder for where you'd define the logic for your rule. # For demonstration, let's just add the coordinates as a simple rule.\n",
    "return analyzer.get_value(x, y) + x + y\n",
    "\n",
    "# Currying the example rule\n",
    "\n",
    "curried_example_rule = curry_rule(example_rule)\n",
    "\n",
    "# Example of extending the class with a new method\n",
    "\n",
    "class ExtendedGridAnalyzer(GridAnalyzer):\n",
    "def apply_custom_rule(self, curried_rule):\n",
    "return self.apply_rule(curried_rule(0, 0)(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow keras numpy scipy scikit-learn pandas pillow h5py pydot graphviz opencv-python flask gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GridAnalyzer:\n",
    "    def __init__(self, filepath):\n",
    "        self.grid = self._read_grid_from_file(filepath)\n",
    "        if self.grid.shape != (100, 100):\n",
    "            raise ValueError(\"Grid must be 100x100.\")\n",
    "\n",
    "    def _read_grid_from_file(self, filepath):\n",
    "        try:\n",
    "            return pd.read_csv(filepath, header=None).values\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File '{filepath}' not found.\")\n",
    "\n",
    "    def get_value(self, x, y):\n",
    "        return self.grid[x, y]\n",
    "\n",
    "    def first_row_values(self, length=10):\n",
    "        return self.grid[0, :length]\n",
    "\n",
    "    def first_column_values(self, length=10):\n",
    "        return self.grid[:length, 0]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_fibonacci(sequence):\n",
    "        fib_seq = [0, 1]\n",
    "        while fib_seq[-1] + fib_seq[-2] <= sequence[-1]:\n",
    "            fib_seq.append(fib_seq[-1] + fib_seq[-2])\n",
    "        return np.array_equal(sequence, fib_seq[-len(sequence):])\n",
    "\n",
    "    def apply_rule(self, rule_func):\n",
    "        transformed_grid = np.zeros_like(self.grid, dtype=float)\n",
    "        for x in range(self.grid.shape[0]):\n",
    "            for y in range(self.grid.shape[1]):\n",
    "                transformed_grid[x, y] = rule_func(self, x, y)\n",
    "        return transformed_grid\n",
    "\n",
    "    @staticmethod\n",
    "    def curry_rule(base_rule):\n",
    "        def curried_rule(x, y):\n",
    "            def apply_rule(grid_analyzer):\n",
    "                return base_rule(grid_analyzer, x, y)\n",
    "            return apply_rule\n",
    "        return curried_rule\n",
    "\n",
    "    @staticmethod\n",
    "    def example_rule(analyzer, x, y):\n",
    "        return analyzer.get_value(x, y) + x + y\n",
    "\n",
    "class ExtendedGridAnalyzer(GridAnalyzer):\n",
    "    def apply_custom_rule(self, curried_rule):\n",
    "        return self.apply_rule(curried_rule(0, 0)(self))\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = 'a.csv'  # Specify your file path here\n",
    "    grid_analyzer = GridAnalyzer(filepath)\n",
    "    # Now you can use grid_analyzer to perform operations\n",
    "    print(grid_analyzer.first_row_values())\n",
    "    print(grid_analyzer.first_column_values())\n",
    "    transpose_results = np.transpose(grid_analyzer.first_row_values(), grid_analyzer.first_column_values())\n",
    "    print(transpose_results)\n",
    "    print(grid_analyzer.is_fibonacci([1, 1, 2, 3, 5, 8, 13]))\n",
    "    print(grid_analyzer.apply_rule(GridAnalyzer.example_rule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Notebook of Examples and Notes\n",
    "\n",
    "# Example 1: Mapping with numeric keys\n",
    "numeric_keys_array = [0, 1, 2, 3, 4]\n",
    "numeric_input_array = [10, 20, 30, 40, 50]\n",
    "\n",
    "print(\"Example 1: Mapping with numeric keys\")\n",
    "for key in numeric_keys_array:\n",
    "    value = numeric_input_array[key]\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Mapping with string keys\n",
    "string_keys_array = ['apple', 'banana', 'orange', 'grape', 'kiwi']\n",
    "string_input_array = [100, 200, 300, 400, 500]\n",
    "\n",
    "print(\"Example 2: Mapping with string keys\")\n",
    "for key in string_keys_array:\n",
    "    value = string_input_array[string_keys_array.index(key)]  # Less efficient method\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Mapping with mixed data types\n",
    "mixed_keys_array = [True, 'b', 2.5, 3, 'elephant']\n",
    "mixed_input_array = ['Yes', 'No', 'Maybe', 'Always', 'Never']\n",
    "\n",
    "print(\"Example 3: Mapping with mixed data types\")\n",
    "for key in mixed_keys_array:\n",
    "    try:\n",
    "        value = mixed_input_array[mixed_keys_array.index(key)]  # Less efficient method\n",
    "        print(f\"Key: {key}, Value: {value}\")\n",
    "    except ValueError:\n",
    "        print(f\"No matching value found for key: {key}\")\n",
    "print()\n",
    "\n",
    "# Example 4: Performing calculations\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "multipliers = [2, 3, 4, 5, 6]\n",
    "\n",
    "print(\"Example 4: Performing calculations\")\n",
    "for index, num in enumerate(numbers):\n",
    "    multiplier = multipliers[index]\n",
    "    result = num * multiplier\n",
    "    print(f\"{num} multiplied by {multiplier} equals {result}\")\n",
    "print()\n",
    "\n",
    "# Example 5: Notes on when to use\n",
    "print(\"Example 5: Notes on when to use\")\n",
    "print(\"- Use this technique when you have two related arrays where elements at corresponding indices or keys are related.\")\n",
    "print(\"- It's efficient for small to medium-sized arrays. For large arrays, consider using more optimized data structures or algorithms.\")\n",
    "print(\"- Ensure that the keys or indices in the first array are valid for accessing elements in the second array to avoid errors.\")\n",
    "print(\"- This technique is versatile and can be applied to various data types including numeric, string, boolean, etc.\")\n",
    "print(\"- Avoid using the less efficient method of finding indices within loops for large arrays; instead, use enumerate().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Notebook of Examples and Notes\n",
    "\n",
    "# Example 1: Mapping with numeric keys\n",
    "numeric_keys_array = [0, 1, 2, 3, 4]\n",
    "numeric_input_array = [10, 20, 30, 40, 50]\n",
    "\n",
    "print(\"Example 1: Mapping with numeric keys\")\n",
    "for key in numeric_keys_array:\n",
    "    value = numeric_input_array[key]\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Mapping with string keys\n",
    "string_keys_array = ['apple', 'banana', 'orange', 'grape', 'kiwi']\n",
    "string_input_array = [100, 200, 300, 400, 500]\n",
    "\n",
    "print(\"Example 2: Mapping with string keys\")\n",
    "for key in string_keys_array:\n",
    "    value = string_input_array[string_keys_array.index(key)]  # Less efficient method\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Mapping with mixed data types\n",
    "mixed_keys_array = [True, 'b', 2.5, 3, 'elephant']\n",
    "mixed_input_array = ['Yes', 'No', 'Maybe', 'Always', 'Never']\n",
    "\n",
    "print(\"Example 3: Mapping with mixed data types\")\n",
    "for key in mixed_keys_array:\n",
    "    try:\n",
    "        value = mixed_input_array[mixed_keys_array.index(key)]  # Less efficient method\n",
    "        print(f\"Key: {key}, Value: {value}\")\n",
    "    except ValueError:\n",
    "        print(f\"No matching value found for key: {key}\")\n",
    "print()\n",
    "\n",
    "# Example 4: Performing calculations\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "multipliers = [2, 3, 4, 5, 6]\n",
    "\n",
    "print(\"Example 4: Performing calculations\")\n",
    "for index, num in enumerate(numbers):\n",
    "    multiplier = multipliers[index]\n",
    "    result = num * multiplier\n",
    "    print(f\"{num} multiplied by {multiplier} equals {result}\")\n",
    "print()\n",
    "\n",
    "# Example 5: Notes on when to use\n",
    "print(\"Example 5: Notes on when to use\")\n",
    "print(\"- Use this technique when you have two related arrays where elements at corresponding indices or keys are related.\")\n",
    "print(\"- It's efficient for small to medium-sized arrays. For large arrays, consider using more optimized data structures or algorithms.\")\n",
    "print(\"- Ensure that the keys or indices in the first array are valid for accessing elements in the second array to avoid errors.\")\n",
    "print(\"- This technique is versatile and can be applied to various data types including numeric, string, boolean, etc.\")\n",
    "print(\"- Avoid using the less efficient method of finding indices within loops for large arrays; instead, use enumerate().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample array\n",
    "my_array = [1, 2, 3, \"apple\"]\n",
    "\n",
    "# Serialize to JSON\n",
    "json_data = json.dumps(my_array)\n",
    "\n",
    "# Encode to bytes\n",
    "binary_data = json_data.encode()\n",
    "\n",
    "print(binary_data)\n",
    "\n",
    "print(\"Original array:\", my_array)\n",
    "print(\"Serialized JSON:\", json_data)\n",
    "print(\"Encoded binary:\", binary_data)\n",
    "\n",
    "\n",
    "\n",
    "# Store in database (Replace with your database interaction code)\n",
    "# cursor.execute(\"INSERT INTO my_table (array_data) VALUES (%s)\", (binary_data,))\n",
    "# connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "G.add_node(1)\n",
    "G.add_nodes_from([2, 3])\n",
    "\n",
    "# Add edges\n",
    "G.add_edge(1, 2)\n",
    "G.add_edges_from([(2, 3), (1, 3)])\n",
    "\n",
    "# Visualize the graph\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.title(\"Simple Graph Visualization\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def create_graph_from_list(lst):\n",
    "    \"\"\"\n",
    "    Create a graph from a 1D list of integers.\n",
    "    Each element in the list represents a node,\n",
    "    and the value represents the weight of the edge.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for i, weight in enumerate(lst):\n",
    "        if i < len(lst) - 1:\n",
    "            G.add_edge(i, i + 1, weight=weight)\n",
    "    return G\n",
    "\n",
    "def visualize_shortest_path(lst, shortest_path):\n",
    "    \"\"\"\n",
    "    Visualize the graph and highlight the shortest path.\n",
    "    \"\"\"\n",
    "    G = create_graph_from_list(lst)\n",
    "    pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1000)\n",
    "\n",
    "    # Highlight the shortest path\n",
    "    edges = [(shortest_path[i], shortest_path[i+1]) for i in range(len(shortest_path)-1)]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=shortest_path, node_color='red', node_size=1000)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color='red', width=3)\n",
    "\n",
    "    # Display labels\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "    plt.title(\"Shortest Path Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "def shortest_path(lst, start, end):\n",
    "    \"\"\"\n",
    "    Compute the shortest path between start and end nodes in the graph.\n",
    "    \"\"\"\n",
    "    G = create_graph_from_list(lst)\n",
    "    shortest_path = nx.shortest_path(G, source=start, target=end)\n",
    "    return shortest_path\n",
    "\n",
    "# Example usage\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "input_list = pd.read_csv(filepath, header=None).values.flatten()\n",
    "start_node = 1\n",
    "end_node = 5\n",
    "\n",
    "shortest = shortest_path(input_list, start_node, end_node)\n",
    "print(\"Shortest Path:\", shortest)\n",
    "\n",
    "visualize_shortest_path(input_list, shortest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursion\n",
    "\n",
    "def fib(n):\n",
    "    if n <= 2 :\n",
    "        return 1\n",
    "    return fib(n-1) + fib(n-2)\n",
    "    \n",
    "# Example usage\n",
    "n = 3\n",
    "print(f\"The {n}th Fibonacci number is {fib(n)}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load sample dataset (Boston house prices dataset)\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple linear regression model without preprocessing\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse_before_preprocessing = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (before preprocessing):\", mse_before_preprocessing)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), list(range(X.shape[1])))  # Standardize all numeric features\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessing pipeline to training data and transform training and testing data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a linear regression model on preprocessed data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_processed, y_train)\n",
    "y_pred_processed = model.predict(X_test_processed)\n",
    "mse_after_preprocessing = mean_squared_error(y_test, y_pred_processed)\n",
    "print(\"Mean Squared Error (after preprocessing):\", mse_after_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Red']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode the 'Color' variable\n",
    "one_hot_encoded = pd.get_dummies(df['Color'], prefix='Color')\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'Color' column\n",
    "df_encoded.drop('Color', axis=1, inplace=True)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_openml\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_dataset():\n",
    "    logger.info(\"Loading MNIST dataset...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X):\n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    # Scale the features between 0 and 1\n",
    "    X_scaled = X / 255.0\n",
    "    return X_scaled\n",
    "\n",
    "def encode_labels(y):\n",
    "    logger.info(\"Performing one-hot encoding on the labels...\")\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "    return y_encoded\n",
    "\n",
    "def split_data(X, y):\n",
    "    logger.info(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    logger.info(\"Training logistic regression model...\")\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    logger.info(\"Making predictions on the testing set...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f\"Accuracy on the testing set: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def plot_learned_weights(model):\n",
    "    logger.info(\"Plotting learned weights for each class...\")\n",
    "    # Get the learned weights (coefficients) of the logistic regression model\n",
    "    weights = model.coef_\n",
    "    \n",
    "    # Plot the learned weights for each class (digit)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(weights[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'Weight for digit {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    X, y = load_dataset()\n",
    "    X_scaled = preprocess_data(X)\n",
    "    y_encoded = encode_labels(y)\n",
    "    X_train, X_test, y_train, y_test = split_data(X_scaled, y_encoded)\n",
    "    model = train_model(X_train, y_train)\n",
    "    accuracy = evaluate_model(model, X_test, y_test)\n",
    "    plot_learned_weights(model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveModel:\n",
    "    def __init__(self, intercept, coefficient, error_correction):\n",
    "        self.intercept = intercept\n",
    "        self.coefficient = coefficient\n",
    "        self.error_correction = error_correction\n",
    "        self.previous_output = None\n",
    "\n",
    "    def predict(self):\n",
    "        if self.previous_output is None:\n",
    "            raise ValueError(\"Previous output is not available. Provide initial value.\")\n",
    "        prediction = self.intercept + self.coefficient * self.previous_output + self.error_correction\n",
    "        return prediction\n",
    "\n",
    "    def update(self, new_output):\n",
    "        self.previous_output = new_output\n",
    "\n",
    "# Example usage:\n",
    "intercept = 1.5\n",
    "coefficient = 0.8\n",
    "error_correction = 0.2\n",
    "\n",
    "# Create an Auto-Regressive Model instance\n",
    "ar_model = AutoRegressiveModel(intercept, coefficient, error_correction)\n",
    "\n",
    "# Provide initial value\n",
    "ar_model.update(2.0)\n",
    "\n",
    "# Predict next output\n",
    "prediction = ar_model.predict()\n",
    "print(\"Next prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input features (previous output)\n",
    "X = np.array([[2], [4], [6], [8], [10]])\n",
    "\n",
    "# Target variable (current prediction)\n",
    "Y = np.array([[3], [7], [11], [15], [19]])\n",
    "\n",
    "# Add intercept term to input features\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Estimate coefficient using vectorized linear regression\n",
    "beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ Y\n",
    "\n",
    "# Extract coefficient (slope)\n",
    "coefficient = beta[1][0]\n",
    "\n",
    "# Absolute value to find magnitude\n",
    "magnitude = abs(coefficient)\n",
    "\n",
    "print(\"Magnitude of coefficient:\", magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input features (previous output)\n",
    "X = np.array([[2], [4], [6], [8], [10]])\n",
    "\n",
    "# Target variable (current prediction)\n",
    "Y = np.array([[3], [7], [11], [15], [19]])\n",
    "\n",
    "# Add intercept term to input features\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Estimate coefficient using vectorized linear regression\n",
    "beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ Y\n",
    "\n",
    "# Extract coefficient (slope)\n",
    "coefficient = beta[1][0]\n",
    "\n",
    "# Absolute value to find magnitude\n",
    "magnitude = abs(coefficient)\n",
    "\n",
    "print(\"Magnitude of coefficient:\", magnitude)\n",
    "\n",
    "# Create a vector v as an example\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Create multiple new variables Z1, Z2, Z3, etc., using different magnitudes\n",
    "Zs = []\n",
    "for i in range(1, 6):\n",
    "    Zs.append(magnitude * (v ** i))\n",
    "\n",
    "# Print the new variables\n",
    "for i, Z in enumerate(Zs, start=1):\n",
    "    print(f\"New variable Z{i}:\", Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input features (previous output)\n",
    "X = np.array([[2], [4], [6], [8], [10]])\n",
    "\n",
    "# Target variable (current prediction)\n",
    "Y = np.array([[3], [7], [11], [15], [19]])\n",
    "\n",
    "# Add intercept term to input features\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Estimate coefficient using vectorized linear regression\n",
    "beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ Y\n",
    "\n",
    "# Extract coefficient (slope)\n",
    "coefficient = beta[1][0]\n",
    "\n",
    "# Absolute value to find magnitude\n",
    "magnitude = abs(coefficient)\n",
    "\n",
    "print(\"Magnitude of coefficient:\", magnitude)\n",
    "\n",
    "# Create a vector v as an example\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Create multiple new variables Z1, Z2, Z3, etc., using different magnitudes\n",
    "Zs = []\n",
    "for i in range(1, 6):\n",
    "    Zs.append(magnitude * (v ** i))\n",
    "\n",
    "# Print the new variables\n",
    "for i, Z in enumerate(Zs, start=1):\n",
    "    print(f\"New variable Z{i}:\", Z)\n",
    "\n",
    "# Create base grid\n",
    "base_grid = np.ones((100, 100), dtype=int) * 20  # Fill with a constant value for demonstration\n",
    "\n",
    "# Save the base grid to a text file\n",
    "np.savetxt('base_grid.txt', base_grid, fmt='%d')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot each vector as bars of varying heights\n",
    "for i, Z in enumerate(Zs, start=1):\n",
    "    ax.bar(np.arange(len(Z)) + i, Z, label=f'Z{i}', alpha=0.7)\n",
    "\n",
    "# Plot base grid\n",
    "ax.imshow(base_grid, cmap='gray', extent=[0, len(Z), 0, 100], alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Visualization of New Variables with Base Grid')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Input features (previous output)\n",
    "X = np.array([])\n",
    "\n",
    "# Target variable (current prediction)\n",
    "Y = np.array([[3], [7], [11], [15], [19]])\n",
    "\n",
    "# Add intercept term to input features\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Estimate coefficient using vectorized linear regression\n",
    "beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ Y\n",
    "\n",
    "# Extract coefficient (slope)\n",
    "coefficient = beta[1][0]\n",
    "\n",
    "# Absolute value to find magnitude\n",
    "magnitude = abs(coefficient)\n",
    "\n",
    "print(\"Magnitude of coefficient:\", magnitude)\n",
    "\n",
    "# Create a vector v as an example\n",
    "v = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Create multiple new variables Z1, Z2, Z3, etc., using different magnitudes\n",
    "Zs = []\n",
    "for i in range(1, 6):\n",
    "    Zs.append(magnitude * (v ** i))\n",
    "\n",
    "# Print the new variables\n",
    "for i, Z in enumerate(Zs, start=1):\n",
    "    print(f\"New variable Z{i}:\", Z)\n",
    "\n",
    "# Create base grid\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "input_list = pd.read_csv(filepath, header=None).values.flatten() \n",
    "# Save the base grid to a text file\n",
    "# np.savetxt('base_grid.txt', input_list, fmt='%d')\n",
    "\n",
    "base_grid = input_list  # Fill with a constant value for demonstration\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot each vector as bars of varying heights\n",
    "for i, Z in enumerate(Zs, start=1):\n",
    "    ax.bar(np.arange(len(Z)) + i, Z, label=f'Z{i}', alpha=0.7)\n",
    "\n",
    "# Plot base grid\n",
    "ax.imshow(base_grid, cmap='gray', extent=[0, len(Z), 0, 100], alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Visualization of New Variables with Base Grid')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "\n",
    "data = pd.read_csv(filepath, header=None).T.to_dict('list' )\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "data = pd.read_csv(filepath, header=None)\n",
    "\n",
    "# Define the length of each vector\n",
    "vector_length = 3  # Adjust this based on your data\n",
    "\n",
    "# Convert the DataFrame to a flat normal array\n",
    "flat_array = data.values.flatten()\n",
    "\n",
    "# Reshape the flat array into vectors of fixed length\n",
    "num_vectors = len(flat_array) // vector_length\n",
    "vectors = flat_array[:num_vectors * vector_length].reshape((num_vectors, vector_length))\n",
    "\n",
    "# Print the vectors\n",
    "# print(\"Vectors:\")\n",
    "# for i, vector in enumerate(vectors):\n",
    "#     print(f\"Vector {i+1}: {vector}\")\n",
    "    \n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(vectors)\n",
    "\n",
    "# Create a DataFrame for the principal components\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plot the principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(principal_df['PC1'], principal_df['PC2'])\n",
    "plt.title('Principal Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = {'ID': [1, 2, 3],\n",
    "        'Name': ['John', 'Alice', 'Bob'],\n",
    "        'Age': [30, 25, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Setting 'ID' column as the index\n",
    "df.set_index('ID', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a set of numbers in which each number is the sum of the two numbers before it. This sequence has a mathematical pattern that repeats forever and has been studied a lot by mathematicians and computer scientists. This post will show you how to make the Fibonacci sequence in Python using different code methods.\n",
    "\n",
    "Introduction to Fibonacci Sequence\n",
    "Most often, the Fibonacci series starts with 0 and 1. Each number after that is the sum of the two numbers before it. 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, etc. In the 1300s, the Italian scientist Fibonacci used it to figure out how rabbits reproduce.\n",
    "\n",
    "The Fibonacci sequence can be defined as:\n",
    "\n",
    "F(n) = F(n-1) + F(n-2)\n",
    "\n",
    "Where F(n) is nth Fibonacci number and F(n-1) and F(n-2) are the recent numbers. This recurrence relation produces the infinite Fibonacci sequence.\n",
    "\n",
    "Due to its simple definition yet complex and endless pattern, the Fibonacci sequence has been studied in depth and applied in diverse fields.\n",
    "\n",
    "Generating Fibonacci Series in Python\n",
    "There are many ways to write the Fibonacci series program in python for as many terms as you want. Let's look at some of the most popular ways.\n",
    "\n",
    "Using a For Loop\n",
    "The simplest method is to use a for loop in Python to calculate and print each term in the Fibonacci sequence iteratively.\n",
    "\n",
    "We initialize two variables a and b with 0 and 1 which represent the starting numbers. Then, use a for loop to iterate up to the number of terms required. We add the previous two terms inside the loop to generate the following time and print it. The loop continues to calculate each subsequent period using this logic.\n",
    "\n",
    "a, b = 0, 1\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "print(a)\n",
    "\n",
    "a, b = b, a + b\n",
    "\n",
    "This will print the first n terms of the Fibonacci sequence. The advantage is the straightforward logic using a basic for-loop construct.\n",
    "\n",
    "Fibonacci Series Using a While Loop\n",
    "The simplest way to print Fibonacci numbers is using a while loop in Python. We initialize two variables, a and b, with 0 and 1,, representing the series' starting numbers. Inside the while loop, we print the current term and update the variables by adding them. This continues recursively to generate the sequence.\n",
    "\n",
    "a, b = 0, 1\n",
    "\n",
    "n = 10\n",
    "\n",
    "while b < n:\n",
    "\n",
    "    print(b)\n",
    "\n",
    "    a, b = b, a+b\n",
    "\n",
    "The loop runs until the term exceeds n and prints the series of up to 10 terms. The while loop method provides a straightforward iterative way to create the Fibonacci sequence.\n",
    "\n",
    "Backtracking Fibonacci Generation\n",
    "Backtracking provides another recursive approach by trying different solutions till the base case is reached.\n",
    "\n",
    "def fib(n, a=0, b=1):\n",
    "\n",
    "    if n == 0:\n",
    "\n",
    "        return a\n",
    "\n",
    "    return fib(n-1, b, a+b)\n",
    "\n",
    "print(fib(5))\n",
    "\n",
    "Thus, techniques like loops, recursion, dynamic programming, and backtracking can efficiently generate the Fibonacci sequence in Python.\n",
    "\n",
    "Using Recursion\n",
    "Recursion is an elegant way to generate the Fibonacci series in Python. We define a recursive function that calls itself to find the following number in the sequence.\n",
    "\n",
    "def fib(n):\n",
    "\n",
    "if n <= 1:\n",
    "\n",
    "       return n\n",
    "\n",
    "else:\n",
    "\n",
    "       return fib(n-1) + fib(n-2)\n",
    "\n",
    "print(fib(7))\n",
    "\n",
    "The fib() function calls itself recursively to calculate the nth term by adding the (n-1)th and (n-2)th terms. This follows the mathematical definition of the Fibonacci sequence.\n",
    "\n",
    "Recursion provides a simple and straightforward way to generate the series. However, it can be slow for more significant inputs due to repeated function calls.\n",
    "\n",
    "Using Dynamic Programming\n",
    "We can optimize the recursive solution using dynamic programming and memoization techniques. The basic idea is to store already computed terms in a lookup table. Before adding any term, we check if it exists in the lookup table. This avoids recomputing the words and makes the algorithm faster.\n",
    "\n",
    "memo = {0:0, 1:1}\n",
    "\n",
    "def fib_dynamic(n):\n",
    "\n",
    "    if n in memo:\n",
    "\n",
    "        return memo[n]\n",
    "\n",
    "    memo[n] = fib_dynamic(n-1) + fib_dynamic(n-2)\n",
    "\n",
    "    return memo[n]\n",
    "\n",
    "print(fib_dynamic(6))\n",
    "\n",
    "We initialize a dictionary memo to store the Fibonacci numbers. The function first checks if the term already exists in a memo before computing it. This dynamic programming approach improves efficiency.\n",
    "\n",
    "Using Caching\n",
    "The Python lru_cache decorator can cache and reuse previously computed Fibonacci terms. This also prevents redundant calculations.\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "\n",
    "def fib(n):\n",
    "\n",
    "    if n == 0:\n",
    "\n",
    "        return 0\n",
    "\n",
    "    elif n == 1:\n",
    "\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        return fib(n-1) + fib(n-2)\n",
    "\n",
    "print(fib(5))\n",
    "\n",
    "The @lru_cache caches the function output. So, any repeated arguments reuse the cached return value, improving performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = {0:0, 1:1} \n",
    "\n",
    "def fib_dynamic(n):\n",
    "\n",
    "    if n in memo:\n",
    "\n",
    "        return memo[n]\n",
    "\n",
    "    memo[n] = fib_dynamic(n-1) + fib_dynamic(n-2)   \n",
    "\n",
    "    return memo[n]\n",
    "\n",
    "print(fib_dynamic(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n, a=0, b=1):\n",
    "\n",
    "    if n == 0: \n",
    "\n",
    "        return a\n",
    "\n",
    "    return fib(n-1, b, a+b)\n",
    "\n",
    "print(fib(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n, a=0, b=1):\n",
    "    if n == 0:\n",
    "        return a\n",
    "    return fib(n-1, b, a+b)\n",
    "print(fib(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_generator(length):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(length):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\n",
    "length = 112\n",
    "fib_sequence = list(fib_generator(length))\n",
    "print(fib_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_generator(length):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(length):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\n",
    "length = 5\n",
    "fib_sequence = list(fib_generator(length))\n",
    "print(fib_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_generator(length):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(length):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\n",
    "length = 10\n",
    "fib_sequence = list(fib_generator(length))\n",
    "print(fib_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting.py\n",
    "from random import sample\n",
    "\n",
    "# List of 1 000 000 integers randomly shuffled\n",
    "MILLION_RANDOM_NUMBERS = sample(range(1_000_000), 1_000_000)\n",
    "\n",
    "\n",
    "def test_sort():\n",
    "    return MILLION_RANDOM_NUMBERS.sort()\n",
    "\n",
    "def test_sorted():\n",
    "    return sorted(MILLION_RANDOM_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "\n",
    "# create a list of dictionaries\n",
    "data = [\n",
    "    {'name': 'Alice', 'age': 25, 'city': 'New York'},\n",
    "    {'name': 'Bob', 'age': 30, 'city': 'Los Angeles'},\n",
    "    {'name': 'Charlie', 'age': 35, 'city': 'Chicago'}\n",
    "]\n",
    "\n",
    "# create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# create a template\n",
    "template = Template('My name is $name and I am $age years old. I live in $city.')\n",
    "\n",
    "# apply the template to each row of the DataFrame\n",
    "df['description'] = df.apply(lambda row: template.substitute(row), axis=1)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the base grid from a CSV file\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "base_grid = pd.read_csv(filepath, header=None)\n",
    "\n",
    "# Create a template\n",
    "template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Apply the template to each cell in the base grid\n",
    "for x in range(base_grid.shape[0]):\n",
    "    for y in range(base_grid.shape[1]):\n",
    "        value = base_grid.iloc[x, y]\n",
    "        print(template.substitute(x=x, y=y, value=value))\n",
    "        \n",
    "\n",
    "\n",
    "# create a list of dictionaries\n",
    "data = [\n",
    "    {'name': 'Alice', 'age': 25, 'city': 'New York'},\n",
    "    {'name': 'Bob', 'age': 30, 'city': 'Los Angeles'},\n",
    "    {'name': 'Charlie', 'age': 35, 'city': 'Chicago'}\n",
    "]\n",
    "\n",
    "# create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# create a template\n",
    "template = Template('My name is $name and I am $age years old. I live in $city.')\n",
    "\n",
    "# apply the template to each row of the DataFrame\n",
    "df['description'] = df.apply(lambda row: template.substitute(row), axis=1)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "# load in a list of well known unstructured dictionaries\n",
    "from sklearn.datasets import load_iris\n",
    "# Read in the base grid from a CSV file\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "base_grid = pd.read_csv(filepath, header=None)\n",
    "\n",
    "# create a dictionary from the base grid\n",
    "grid_dict = base_grid.to_dict()\n",
    "\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# create a DataFrame\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# display the first few rows of the DataFrame\n",
    "print(df)\n",
    "\n",
    "# transpose the DataFrame to switch rows and columns\n",
    "df_transposed = df.T\n",
    "\n",
    "# display the transposed DataFrame\n",
    "print(df_transposed)\n",
    "\n",
    "# # create a list of dictionaries\n",
    "# data = [\n",
    "#     {'name': 'Alice', 'age': 25, 'city': 'New York'},\n",
    "#     {'name': 'Bob', 'age': 30, 'city': 'Los Angeles'},\n",
    "#     {'name': 'Charlie', 'age': 35, 'city': 'Chicago'}\n",
    "# ]\n",
    "\n",
    "# # create a DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # create a template\n",
    "# template = Template('My name is $name and I am $age years old. I live in $city.')\n",
    "\n",
    "# # apply the template to each row of the DataFrame\n",
    "# df['description'] = df.apply(lambda row: template.substitute(row), axis=1)\n",
    "\n",
    "# # display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "# load in a list of well known unstructured dictionaries\n",
    "from sklearn.datasets import load_iris\n",
    "# Read in the base grid from a CSV file\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "base_grid = pd.read_csv(filepath, header=None)\n",
    "\n",
    "# flatten the base grid into a 1D array\n",
    "flat_array = base_grid.values.flatten()\n",
    "\n",
    "\n",
    "\n",
    "# # apply the template to each element in the flat array\n",
    "# for i, value in enumerate(flat_array):\n",
    "#     print(template.substitute(index=i, value=value))\n",
    "  \n",
    "# create a large array of names and ages and full sentences\n",
    "example_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "    \"The sun sets in the west, painting the sky with hues of orange and pink.\",\n",
    "    \"It was a dark and stormy night; the wind howled outside the window.\",\n",
    "    \"She walked through the forest, listening to the birds chirping and leaves rustling under her feet.\",\n",
    "    \"The scientist conducted experiments to test their hypothesis and analyze the results.\",\n",
    "    \"In ancient times, civilizations thrived along the banks of great rivers such as the Nile and the Tigris.\",\n",
    "    \"He gazed up at the stars, pondering the vastness of the universe and his place within it.\",\n",
    "    \"The smell of freshly baked bread wafted through the air, enticing passersby to the bakery.\",\n",
    "    \"They danced under the moonlight, lost in the rhythm of the music and the embrace of each other.\",\n",
    "    \"The city bustled with activity as people hurried to work and children played in the streets.\",\n",
    "    \"The painting depicted a serene landscape, with mountains in the distance and a tranquil lake in the foreground.\",\n",
    "    \"As the seasons changed, so did the colors of the leaves, turning from green to red, orange, and yellow.\",\n",
    "    \"She savored the taste of ripe strawberries, their sweetness bursting with every bite.\",\n",
    "    \"The bookshelf was filled with a diverse collection of literature, from classic novels to modern poetry.\",\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\",\n",
    "    \"The laughter of children playing in the park filled the air, a joyful symphony of innocence and happiness.\"\n",
    "]\n",
    "\n",
    "# Print the array to verify\n",
    "#print(example_sentences)\n",
    "\n",
    "# # explode the array of sentances into individual letters and a fully flat array\n",
    "# flat_array = [char for sentance in example_sentences for char in sentance]\n",
    "\n",
    "# # Print the flat array to verify\n",
    "# print(flat_array)\n",
    "\n",
    "# # addign\n",
    "    \n",
    "# explode the array of sentances into individual letters and a fully flat array\n",
    "atomic_layer = [char for sentance in example_sentences for char in sentance]\n",
    "\n",
    "#print(atomic_layer)\n",
    "\n",
    "# assign index and value (index,value) to create a flat_atomic_layer map\n",
    "flat_atomic_layer = {index: value for index, value in enumerate(atomic_layer)}\n",
    "\n",
    "#print(flat_atomic_layer)\n",
    "\n",
    "# explode the sentances into individual words and a fully flat array\n",
    "word_layer = [word for sentance in example_sentences for word in sentance.split()]\n",
    "\n",
    "##print(word_layer)\n",
    "\n",
    "# assign index and value (index,value) to create full word map\n",
    "flat_word_layer = {index: value for index, value in enumerate(word_layer)}\n",
    "\n",
    "#print(flat_word_layer)\n",
    "\n",
    "# create a dictionary of the full sentences\n",
    "sentance_layer = {index: value for index, value in enumerate(example_sentences)}\n",
    "\n",
    "#print(sentance_layer)\n",
    "\n",
    "# now use the index of each layer digitwise to assign to cells in the base grid layer and create a 3D grid\n",
    "# create a 3D grid\n",
    "grid = {0: flat_atomic_layer, 1: flat_word_layer, 2: sentance_layer}\n",
    "\n",
    "#print(grid)\n",
    "\n",
    "\n",
    "collapse = {index: value for index, value in enumerate(flat_array)}\n",
    "\n",
    "print(collapse)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the base grid from a CSV file\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "base_grid = pd.read_csv(filepath, header=None)\n",
    "\n",
    "# Create a template\n",
    "template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "print(base_grid)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the base grid from a CSV file\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "base_grid = pd.read_csv(filepath, header=None)\n",
    "\n",
    "col_names = [f'col_{i}' for i in range(len(base_grid.columns))] \n",
    "row_names = [f'row_{i}' for i in range(len(base_grid.index))]\n",
    "base_grid.columns = col_names\n",
    "base_grid.index = row_names\n",
    "\n",
    "# Create a template\n",
    "template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Function to generate a template for each cell in the grid\n",
    "def generate_templates(grid):\n",
    "    templates = []\n",
    "    for i in range(len(grid.index)):\n",
    "        for j in range(len(grid.columns)):\n",
    "            value = grid.iloc[i, j]\n",
    "            template_str = template.substitute(x=i, y=j, value=value)\n",
    "            templates.append(template_str)\n",
    "    return templates\n",
    "\n",
    "# Generate templates for each cell in the base grid\n",
    "grid_templates = generate_templates(base_grid)\n",
    "\n",
    "# Print the grid templates\n",
    "for template_str in grid_templates:\n",
    "    print(template_str)\n",
    "    \n",
    "\n",
    "example_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "    \"The sun sets in the west, painting the sky with hues of orange and pink.\",\n",
    "    \"It was a dark and stormy night; the wind howled outside the window.\",\n",
    "    \"She walked through the forest, listening to the birds chirping and leaves rustling under her feet.\",\n",
    "    \"The scientist conducted experiments to test their hypothesis and analyze the results.\",\n",
    "    \"In ancient times, civilizations thrived along the banks of great rivers such as the Nile and the Tigris.\",\n",
    "    \"He gazed up at the stars, pondering the vastness of the universe and his place within it.\",\n",
    "    \"The smell of freshly baked bread wafted through the air, enticing passersby to the bakery.\",\n",
    "    \"They danced under the moonlight, lost in the rhythm of the music and the embrace of each other.\",\n",
    "    \"The city bustled with activity as people hurried to work and children played in the streets.\",\n",
    "    \"The painting depicted a serene landscape, with mountains in the distance and a tranquil lake in the foreground.\",\n",
    "    \"As the seasons changed, so did the colors of the leaves, turning from green to red, orange, and yellow.\",\n",
    "    \"She savored the taste of ripe strawberries, their sweetness bursting with every bite.\",\n",
    "    \"The bookshelf was filled with a diverse collection of literature, from classic novels to modern poetry.\",\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\",\n",
    "    \"The laughter of children playing in the park filled the air, a joyful symphony of innocence and happiness.\"\n",
    "]\n",
    "\n",
    "print(example_sentences)\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Function to generate a template for each cell in the grid\n",
    "def generate_input_templates(input_grid):\n",
    "    templates = []\n",
    "    for i in range(len(input_grid)):\n",
    "        sentence = input_grid[i]\n",
    "        template_str = input_template.substitute(value=sentence, x=i, y=0)  # Assuming input is in a single column\n",
    "        templates.append(template_str)\n",
    "    return templates\n",
    "\n",
    "# Generate templates for each input sentence\n",
    "input_templates = generate_input_templates(example_sentences)\n",
    "\n",
    "# Print the input templates\n",
    "for input_template_str in input_templates:\n",
    "    print(input_template_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Create a dictionary to map each unique word to an integer index\n",
    "word_index = {}\n",
    "current_index = 0\n",
    "\n",
    "# Function to tokenize and update word index dictionary\n",
    "def update_word_index(sentence):\n",
    "    global current_index\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Tokenize sentences and update word index dictionary\n",
    "for sentence in example_sentences:\n",
    "    update_word_index(sentence)\n",
    "\n",
    "# Function to perform one-hot encoding for a sentence\n",
    "def one_hot_encode(sentence):\n",
    "    tokens = sentence.split()\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Perform one-hot encoding for each sentence\n",
    "one_hot_encodings = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    encoding = one_hot_encode(sentence)\n",
    "    one_hot_encodings.append(encoding)\n",
    "    print(input_template.substitute(x=i+15, y=0, value=sentence))\n",
    "\n",
    "# Print one-hot encodings\n",
    "for i, encoding in enumerate(one_hot_encodings):\n",
    "    print(f\"One-hot encoding for sentence {i+15}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Create a dictionary to map each unique word to an integer index\n",
    "word_index = {}\n",
    "current_index = 0\n",
    "\n",
    "# Function to tokenize and update word index dictionary\n",
    "def update_word_index(sentence):\n",
    "    global current_index\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Tokenize sentences and update word index dictionary\n",
    "for sentence in example_sentences:\n",
    "    update_word_index(sentence)\n",
    "\n",
    "# Function to perform one-hot encoding for a sentence\n",
    "def one_hot_encode(sentence):\n",
    "    tokens = sentence.split()\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Perform one-hot encoding for each sentence\n",
    "one_hot_encodings = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    encoding = one_hot_encode(sentence)\n",
    "    one_hot_encodings.append(encoding)\n",
    "    print(input_template.substitute(x=i+15, y=0, value=sentence))\n",
    "\n",
    "# Print one-hot encodings\n",
    "for i, encoding in enumerate(one_hot_encodings):\n",
    "    print(f\"One-hot encoding for sentence {i+15}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "input_list = pd.read_csv(filepath, header=None).values\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Create a dictionary to map each unique word to an integer index\n",
    "word_index = {}\n",
    "current_index = 0\n",
    "\n",
    "# Function to tokenize and update word index dictionary\n",
    "def update_word_index(sentence):\n",
    "    global current_index\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Tokenize sentences and update word index dictionary\n",
    "for sentence in example_sentences:\n",
    "    update_word_index(sentence)\n",
    "\n",
    "# Function to perform one-hot encoding for a sentence\n",
    "def one_hot_encode(sentence):\n",
    "    tokens = sentence.split()\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Perform one-hot encoding for each sentence\n",
    "one_hot_encodings = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    encoding = one_hot_encode(sentence)\n",
    "    one_hot_encodings.append(encoding)\n",
    "    print(input_template.substitute(x=i+15, y=0, value=sentence))\n",
    "\n",
    "# Create a DataFrame for one-hot encodings\n",
    "one_hot_df = pd.DataFrame(one_hot_encodings, columns=word_index.keys())\n",
    "\n",
    "# Create a DataFrame of the input list\n",
    "input_df = pd.DataFrame(input_list)\n",
    "\n",
    "# Perform PCA for dimensionality reduction on the input list\n",
    "n_components = min(len(input_df), len(input_df.columns))\n",
    "pca_input = PCA(n_components=n_components)\n",
    "input_pca_result = pca_input.fit_transform(input_df)\n",
    "\n",
    "# Concatenate the base grid with the one-hot DataFrame\n",
    "result_df = pd.concat([input_df, one_hot_df], axis=1).fillna(0.0)\n",
    "\n",
    "# Convert column names to strings\n",
    "result_df.columns = result_df.columns.astype(str)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "n_components = min(len(result_df), len(result_df.columns))\n",
    "pca_result = PCA(n_components=n_components)\n",
    "result_pca_result = pca_result.fit_transform(result_df)\n",
    "\n",
    "# Visualize the reduced data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(input_pca_result[:, 0], input_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Input List')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(result_pca_result[:, 0], result_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Concatenated Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize SQLite database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS InputList (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                value REAL\n",
    "            )''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS OneHotEncodings (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                sentence TEXT\n",
    "            )''')\n",
    "\n",
    "# File path for CSV\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "\n",
    "# Read input data from CSV file\n",
    "input_list = pd.read_csv(filepath, header=None).values\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Create a dictionary to map each unique word to an integer index\n",
    "word_index = {}\n",
    "current_index = 0\n",
    "\n",
    "# Function to tokenize and update word index dictionary\n",
    "def update_word_index(sentence):\n",
    "    global current_index\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Tokenize sentences and update word index dictionary\n",
    "for sentence in example_sentences:\n",
    "    update_word_index(sentence)\n",
    "\n",
    "# Function to perform one-hot encoding for a sentence\n",
    "def one_hot_encode(sentence):\n",
    "    tokens = sentence.split()\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Perform one-hot encoding for each sentence\n",
    "one_hot_encodings = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    encoding = one_hot_encode(sentence)\n",
    "    one_hot_encodings.append(encoding)\n",
    "    print(input_template.substitute(x=i+15, y=0, value=sentence))\n",
    "\n",
    "# Create a DataFrame for one-hot encodings\n",
    "one_hot_df = pd.DataFrame(one_hot_encodings, columns=word_index.keys())\n",
    "\n",
    "# Insert data into InputList table\n",
    "for i, row in enumerate(input_list):\n",
    "    cur.execute('''INSERT INTO InputList (id, value) VALUES (?, ?)''', (i, row[0]))\n",
    "\n",
    "# Insert data into OneHotEncodings table\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    cur.execute('''INSERT INTO OneHotEncodings (id, sentence) VALUES (?, ?)''', (i, sentence))\n",
    "\n",
    "# Save changes and close connection to database\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Create a DataFrame of the input list\n",
    "input_df = pd.DataFrame(input_list)\n",
    "\n",
    "# Perform PCA for dimensionality reduction on the input list\n",
    "n_components = min(len(input_df), len(input_df.columns))\n",
    "pca_input = PCA(n_components=n_components)\n",
    "input_pca_result = pca_input.fit_transform(input_df)\n",
    "\n",
    "# Concatenate the base grid with the one-hot DataFrame\n",
    "result_df = pd.concat([input_df, one_hot_df], axis=1).fillna(0.0)\n",
    "\n",
    "# Convert column names to strings\n",
    "result_df.columns = result_df.columns.astype(str)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "n_components = min(len(result_df), len(result_df.columns))\n",
    "pca_result = PCA(n_components=n_components)\n",
    "result_pca_result = pca_result.fit_transform(result_df)\n",
    "\n",
    "# Visualize the reduced data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(input_pca_result[:, 0], input_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Input List')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(result_pca_result[:, 0], result_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Concatenated Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to preprocess sentences (can be expanded based on specific requirements)\n",
    "def preprocess_sentences(sentences):\n",
    "    return sentences\n",
    "\n",
    "# Function to tokenize sentences and create word index\n",
    "def tokenize_sentences(sentences):\n",
    "    word_index = {}\n",
    "    current_index = 0\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = current_index\n",
    "                current_index += 1\n",
    "    return word_index\n",
    "\n",
    "# Function to perform one-hot encoding\n",
    "def one_hot_encode(sentence, word_index):\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Function to visualize PCA results\n",
    "def visualize_pca(data, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(label='Ground Truth Labels')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Preprocess sentences and tokenize them\n",
    "preprocessed_sentences = preprocess_sentences(example_sentences)\n",
    "word_index = tokenize_sentences(preprocessed_sentences)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encodings = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoding = one_hot_encode(sentence, word_index)\n",
    "    one_hot_encodings.append(encoding)\n",
    "\n",
    "# Create DataFrame for one-hot encodings\n",
    "one_hot_df = pd.DataFrame(one_hot_encodings, columns=word_index.keys())\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(one_hot_df)\n",
    "\n",
    "# Visualize PCA result\n",
    "visualize_pca(pca_result, 'PCA Visualization of Sentence Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from string import Template\n",
    "\n",
    "# Initialize SQLite database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create tables with auto-increment primary key\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS InputList (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                value REAL\n",
    "            )''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS OneHotEncodings (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                sentence TEXT\n",
    "            )''')\n",
    "\n",
    "# File path for CSV\n",
    "filepath = 'a.csv'  # Specify your file path here\n",
    "\n",
    "# Read input data from CSV file\n",
    "input_list = pd.read_csv(filepath, header=None).values\n",
    "\n",
    "# Sample input sentences\n",
    "example_sentences = [\n",
    "    \"The sound of waves crashing against the shore echoed through the empty beach.\",\n",
    "    \"They hiked through the mountains, marveling at the breathtaking views and feeling the crisp mountain air on their faces.\",\n",
    "    \"The smell of fresh coffee greeted her as she entered the cafÃ©, warming her from the inside out.\",\n",
    "    \"He studied diligently for the exam, determined to succeed and achieve his goals.\"\n",
    "]\n",
    "\n",
    "# Create a template for input sentences\n",
    "input_template = Template('The value at position ($x, $y) is $value.')\n",
    "\n",
    "# Create a dictionary to map each unique word to an integer index\n",
    "word_index = {}\n",
    "current_index = 0\n",
    "\n",
    "# Function to tokenize and update word index dictionary\n",
    "def update_word_index(sentence):\n",
    "    global current_index\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Tokenize sentences and update word index dictionary\n",
    "for sentence in example_sentences:\n",
    "    update_word_index(sentence)\n",
    "\n",
    "# Function to perform one-hot encoding for a sentence\n",
    "def one_hot_encode(sentence):\n",
    "    tokens = sentence.split()\n",
    "    encoding = np.zeros(len(word_index))\n",
    "    for token in tokens:\n",
    "        index = word_index[token]\n",
    "        encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Perform one-hot encoding for each sentence\n",
    "one_hot_encodings = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    encoding = one_hot_encode(sentence)\n",
    "    one_hot_encodings.append(encoding)\n",
    "    print(input_template.substitute(x=i+15, y=0, value=sentence))\n",
    "\n",
    "# Create a DataFrame for one-hot encodings\n",
    "one_hot_df = pd.DataFrame(one_hot_encodings, columns=word_index.keys())\n",
    "\n",
    "# Insert data into InputList table\n",
    "for row in input_list:\n",
    "    cur.execute('''INSERT INTO InputList (value) VALUES (?)''', (row[0],))\n",
    "\n",
    "# Insert data into OneHotEncodings table\n",
    "for sentence in example_sentences:\n",
    "    cur.execute('''INSERT INTO OneHotEncodings (sentence) VALUES (?)''', (sentence,))\n",
    "\n",
    "# Save changes and close connection to database\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Create a DataFrame of the input list\n",
    "input_df = pd.DataFrame(input_list)\n",
    "\n",
    "# Perform PCA for dimensionality reduction on the input list\n",
    "n_components = min(len(input_df), len(input_df.columns))\n",
    "pca_input = PCA(n_components=n_components)\n",
    "input_pca_result = pca_input.fit_transform(input_df)\n",
    "\n",
    "# Concatenate the base grid with the one-hot DataFrame\n",
    "result_df = pd.concat([input_df, one_hot_df], axis=1).fillna(0.0)\n",
    "\n",
    "# Convert column names to strings\n",
    "result_df.columns = result_df.columns.astype(str)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "n_components = min(len(result_df), len(result_df.columns))\n",
    "pca_result = PCA(n_components=n_components)\n",
    "result_pca_result = pca_result.fit_transform(result_df)\n",
    "\n",
    "# Visualize the reduced data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(input_pca_result[:, 0], input_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Input List')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(result_pca_result[:, 0], result_pca_result[:, 1], cmap='viridis')\n",
    "plt.title('PCA Visualization of Concatenated Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Print contents of InputList table\n",
    "cur.execute('''SELECT * FROM InputList''')\n",
    "print(\"Contents of InputList table:\")\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Print contents of OneHotEncodings table\n",
    "cur.execute('''SELECT * FROM OneHotEncodings''')\n",
    "print(\"\\nContents of OneHotEncodings table:\")\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Close connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the integer table\n",
    "def create_integer_table(start, end):\n",
    "    integer_table = {}\n",
    "    for i in range(start, end+1):\n",
    "        byte_format = i.to_bytes(8, byteorder='big')  # Convert integer to byte format (8 bytes)\n",
    "        integer_table[i] = byte_format\n",
    "    return integer_table\n",
    "\n",
    "# Call the function to create the integer table for the range 0 to 100\n",
    "integer_table_0_to_100 = create_integer_table(0, 100)\n",
    "\n",
    "# Now you have a dictionary where keys are integers from 0 to 100 and values are their byte format representations\n",
    "# You can access individual entries like this:\n",
    "print(\"Byte format for integer 50:\", integer_table_0_to_100[50])\n",
    "\n",
    "# Or iterate over all entries like this:\n",
    "for integer, byte_format in integer_table_0_to_100.items():\n",
    "    print(f\"{integer}: {byte_format}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(start, end):\n",
    "    # Factorial function\n",
    "    def factorial(n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return n * factorial(n-1)\n",
    "    \n",
    "    # Fibonacci function\n",
    "    def fibonacci(n):\n",
    "        fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "        while len(fibonacci_sequence) < n:\n",
    "            next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "            fibonacci_sequence.append(next_fib)\n",
    "        return fibonacci_sequence\n",
    "    \n",
    "    # Integer table function\n",
    "    def create_integer_table(start, end):\n",
    "        integer_table = {}\n",
    "        for i in range(start, end+1):\n",
    "            byte_format = i.to_bytes(8, byteorder='big')  # Convert integer to byte format (8 bytes)\n",
    "            integer_table[i] = byte_format\n",
    "        return integer_table\n",
    "    \n",
    "    # Create tables\n",
    "    factorial_table = [factorial(i) for i in range(start, end+1)]\n",
    "    fibonacci_table = fibonacci(end)\n",
    "    integer_table = create_integer_table(start, end)\n",
    "    \n",
    "    return factorial_table, fibonacci_table, integer_table\n",
    "\n",
    "# Call the function to create the tables for the range 0 to 100\n",
    "start = 0\n",
    "end = 100\n",
    "factorial_table, fibonacci_table, integer_table = create_tables(start, end)\n",
    "\n",
    "# Print the tables\n",
    "print(\"Factorial table:\")\n",
    "for i, factorial_value in enumerate(factorial_table):\n",
    "    print(f\"Factorial of {i}: {factorial_value}\")\n",
    "\n",
    "print(\"\\nFibonacci table:\")\n",
    "for i, fib_number in enumerate(fibonacci_table):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n",
    "\n",
    "print(\"\\nInteger table:\")\n",
    "for integer, byte_format in integer_table.items():\n",
    "    print(f\"{integer}: {byte_format}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\n",
    "def create_factorial_table(n):\n",
    "    factorial_table = [factorial(i) for i in range(n+1)]\n",
    "    return factorial_table\n",
    "\n",
    "n = 100\n",
    "factorial_table = create_factorial_table(n)\n",
    "\n",
    "# Print the factorial table\n",
    "for i, factorial_value in enumerate(factorial_table):\n",
    "    print(f\"Factorial of {i}: {factorial_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_table(n):\n",
    "    fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "    while len(fibonacci_sequence) < n:\n",
    "        next_fib = fibonacci_sequence[-1] + fibonacci_sequence[-2]  # Calculate the next Fibonacci number\n",
    "        fibonacci_sequence.append(next_fib)\n",
    "    return fibonacci_sequence\n",
    "\n",
    "n = 100  # Number of Fibonacci numbers to generate\n",
    "fibonacci_table_result = fibonacci_table(n)\n",
    "\n",
    "# Print the Fibonacci table\n",
    "for i, fib_number in enumerate(fibonacci_table_result):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_table(n):\n",
    "    fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "    while len(fibonacci_sequence) < n:\n",
    "        next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "        fibonacci_sequence.append(next_fib)\n",
    "    return fibonacci_sequence\n",
    "\n",
    "n = 100  # Number of Fibonacci numbers to generate\n",
    "fibonacci_table_result = fibonacci_table(n)\n",
    "\n",
    "# Print the Fibonacci table\n",
    "for i, fib_number in enumerate(fibonacci_table_result):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_table(n):\n",
    "    fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "    while len(fibonacci_sequence) < n:\n",
    "        next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "        fibonacci_sequence.append(next_fib)\n",
    "    return fibonacci_sequence\n",
    "\n",
    "n = 100  # Number of Fibonacci numbers to generate\n",
    "fibonacci_table_result = fibonacci_table(n)\n",
    "\n",
    "# Print the Fibonacci table\n",
    "for i, fib_number in enumerate(fibonacci_table_result):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_tables(start, end):\n",
    "    # Factorial function\n",
    "    def factorial(n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return n * factorial(n-1)\n",
    "    \n",
    "    # Fibonacci function\n",
    "    def fibonacci(n):\n",
    "        fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "        while len(fibonacci_sequence) < n:\n",
    "            next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "            fibonacci_sequence.append(next_fib)\n",
    "        return fibonacci_sequence\n",
    "    \n",
    "    # Integer table function\n",
    "    def create_integer_table(start, end):\n",
    "        integer_table = {}\n",
    "        for i in range(start, end+1):\n",
    "            byte_format = i.to_bytes(8, byteorder='big')  # Convert integer to byte format (8 bytes)\n",
    "            integer_table[i] = byte_format\n",
    "        return integer_table\n",
    "    \n",
    "    # Create tables\n",
    "    factorial_table = [factorial(i) for i in range(start, end+1)]\n",
    "    fibonacci_table = fibonacci(end)\n",
    "    integer_table = create_integer_table(start, end)\n",
    "    \n",
    "    return factorial_table, fibonacci_table, integer_table\n",
    "\n",
    "# Call the function to create the tables for the range 0 to 100\n",
    "start = 0\n",
    "end = 100\n",
    "factorial_table, fibonacci_table, integer_table = create_tables(start, end)\n",
    "\n",
    "# Print the tables\n",
    "print(\"Factorial table:\")\n",
    "for i, factorial_value in enumerate(factorial_table):\n",
    "    print(f\"Factorial of {i}: {factorial_value}\")\n",
    "\n",
    "print(\"\\nFibonacci table:\")\n",
    "for i, fib_number in enumerate(fibonacci_table):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n",
    "\n",
    "print(\"\\nInteger table:\")\n",
    "for integer, byte_format in integer_table.items():\n",
    "    print(f\"{integer}: {byte_format}\")\n",
    "\n",
    "# Read the pattern of numbers from a CSV file\n",
    "pattern = pd.read_csv('a.csv', header=None).values.flatten()\n",
    "\n",
    "# Create a dictionary to map the pattern numbers to table values\n",
    "mapped_tables = {}\n",
    "for num in pattern:\n",
    "    if num <= end:\n",
    "        mapped_tables[num] = (factorial_table[num], fibonacci_table[num], integer_table[num])\n",
    "\n",
    "# Print the mapped tables\n",
    "for num, values in mapped_tables.items():\n",
    "    print(f\"Number: {num}\")\n",
    "    print(f\"Factorial: {values[0]}\")\n",
    "    print(f\"Fibonacci: {values[1]}\")\n",
    "    print(f\"Integer: {values[2]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_tables(start, end):\n",
    "    # Factorial function\n",
    "    def factorial(n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return n * factorial(n-1)\n",
    "    \n",
    "    # Fibonacci function\n",
    "    def fibonacci(n):\n",
    "        fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "        while len(fibonacci_sequence) < n:\n",
    "            next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "            fibonacci_sequence.append(next_fib)\n",
    "        return fibonacci_sequence\n",
    "    \n",
    "    # Integer table function\n",
    "    def create_integer_table(start, end):\n",
    "        integer_table = {}\n",
    "        for i in range(start, end+1):\n",
    "            byte_format = i.to_bytes(8, byteorder='big')  # Convert integer to byte format (8 bytes)\n",
    "            integer_table[i] = byte_format\n",
    "        return integer_table\n",
    "    \n",
    "    # Create tables\n",
    "    factorial_table = [factorial(i) for i in range(start, end+1)]\n",
    "    fibonacci_table = fibonacci(end)\n",
    "    integer_table = create_integer_table(start, end)\n",
    "    \n",
    "    return factorial_table, fibonacci_table, integer_table\n",
    "\n",
    "# Call the function to create the tables for the range 0 to 100\n",
    "start = 0\n",
    "end = 100\n",
    "factorial_table, fibonacci_table, integer_table = create_tables(start, end)\n",
    "\n",
    "# Print the tables\n",
    "print(\"Factorial table:\")\n",
    "for i, factorial_value in enumerate(factorial_table):\n",
    "    print(f\"Factorial of {i}: {factorial_value}\")\n",
    "\n",
    "print(\"\\nFibonacci table:\")\n",
    "for i, fib_number in enumerate(fibonacci_table):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n",
    "\n",
    "print(\"\\nInteger table:\")\n",
    "for integer, byte_format in integer_table.items():\n",
    "    print(f\"{integer}: {byte_format}\")\n",
    "\n",
    "# Read the pattern of numbers from a CSV file\n",
    "pattern = pd.read_csv('a.csv', header=None).values.flatten()\n",
    "\n",
    "# Create a dictionary to map the pattern numbers to table values\n",
    "mapped_tables = {}\n",
    "for num in pattern:\n",
    "    if num >= start and num <= end:\n",
    "        mapped_tables[num] = (factorial_table[num - start], fibonacci_table[num - start], integer_table[num])\n",
    "\n",
    "# Print the mapped tables\n",
    "for num, values in mapped_tables.items():\n",
    "    print(f\"Number: {num}\")\n",
    "    print(f\"Factorial: {values[0]}\")\n",
    "    print(f\"Fibonacci: {values[1]}\")\n",
    "    print(f\"Integer: {values[2]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorial table:\n",
      "Factorial of 0: 1\n",
      "Factorial of 1: 1\n",
      "Factorial of 2: 2\n",
      "Factorial of 3: 6\n",
      "Factorial of 4: 24\n",
      "Factorial of 5: 120\n",
      "Factorial of 6: 720\n",
      "Factorial of 7: 5040\n",
      "Factorial of 8: 40320\n",
      "Factorial of 9: 362880\n",
      "Factorial of 10: 3628800\n",
      "Factorial of 11: 39916800\n",
      "Factorial of 12: 479001600\n",
      "Factorial of 13: 6227020800\n",
      "Factorial of 14: 87178291200\n",
      "Factorial of 15: 1307674368000\n",
      "Factorial of 16: 20922789888000\n",
      "Factorial of 17: 355687428096000\n",
      "Factorial of 18: 6402373705728000\n",
      "Factorial of 19: 121645100408832000\n",
      "Factorial of 20: 2432902008176640000\n",
      "Factorial of 21: 51090942171709440000\n",
      "Factorial of 22: 1124000727777607680000\n",
      "Factorial of 23: 25852016738884976640000\n",
      "Factorial of 24: 620448401733239439360000\n",
      "Factorial of 25: 15511210043330985984000000\n",
      "Factorial of 26: 403291461126605635584000000\n",
      "Factorial of 27: 10888869450418352160768000000\n",
      "Factorial of 28: 304888344611713860501504000000\n",
      "Factorial of 29: 8841761993739701954543616000000\n",
      "Factorial of 30: 265252859812191058636308480000000\n",
      "Factorial of 31: 8222838654177922817725562880000000\n",
      "Factorial of 32: 263130836933693530167218012160000000\n",
      "Factorial of 33: 8683317618811886495518194401280000000\n",
      "Factorial of 34: 295232799039604140847618609643520000000\n",
      "Factorial of 35: 10333147966386144929666651337523200000000\n",
      "Factorial of 36: 371993326789901217467999448150835200000000\n",
      "Factorial of 37: 13763753091226345046315979581580902400000000\n",
      "Factorial of 38: 523022617466601111760007224100074291200000000\n",
      "Factorial of 39: 20397882081197443358640281739902897356800000000\n",
      "Factorial of 40: 815915283247897734345611269596115894272000000000\n",
      "Factorial of 41: 33452526613163807108170062053440751665152000000000\n",
      "Factorial of 42: 1405006117752879898543142606244511569936384000000000\n",
      "Factorial of 43: 60415263063373835637355132068513997507264512000000000\n",
      "Factorial of 44: 2658271574788448768043625811014615890319638528000000000\n",
      "Factorial of 45: 119622220865480194561963161495657715064383733760000000000\n",
      "Factorial of 46: 5502622159812088949850305428800254892961651752960000000000\n",
      "Factorial of 47: 258623241511168180642964355153611979969197632389120000000000\n",
      "Factorial of 48: 12413915592536072670862289047373375038521486354677760000000000\n",
      "Factorial of 49: 608281864034267560872252163321295376887552831379210240000000000\n",
      "Factorial of 50: 30414093201713378043612608166064768844377641568960512000000000000\n",
      "Factorial of 51: 1551118753287382280224243016469303211063259720016986112000000000000\n",
      "Factorial of 52: 80658175170943878571660636856403766975289505440883277824000000000000\n",
      "Factorial of 53: 4274883284060025564298013753389399649690343788366813724672000000000000\n",
      "Factorial of 54: 230843697339241380472092742683027581083278564571807941132288000000000000\n",
      "Factorial of 55: 12696403353658275925965100847566516959580321051449436762275840000000000000\n",
      "Factorial of 56: 710998587804863451854045647463724949736497978881168458687447040000000000000\n",
      "Factorial of 57: 40526919504877216755680601905432322134980384796226602145184481280000000000000\n",
      "Factorial of 58: 2350561331282878571829474910515074683828862318181142924420699914240000000000000\n",
      "Factorial of 59: 138683118545689835737939019720389406345902876772687432540821294940160000000000000\n",
      "Factorial of 60: 8320987112741390144276341183223364380754172606361245952449277696409600000000000000\n",
      "Factorial of 61: 507580213877224798800856812176625227226004528988036003099405939480985600000000000000\n",
      "Factorial of 62: 31469973260387937525653122354950764088012280797258232192163168247821107200000000000000\n",
      "Factorial of 63: 1982608315404440064116146708361898137544773690227268628106279599612729753600000000000000\n",
      "Factorial of 64: 126886932185884164103433389335161480802865516174545192198801894375214704230400000000000000\n",
      "Factorial of 65: 8247650592082470666723170306785496252186258551345437492922123134388955774976000000000000000\n",
      "Factorial of 66: 544344939077443064003729240247842752644293064388798874532860126869671081148416000000000000000\n",
      "Factorial of 67: 36471110918188685288249859096605464427167635314049524593701628500267962436943872000000000000000\n",
      "Factorial of 68: 2480035542436830599600990418569171581047399201355367672371710738018221445712183296000000000000000\n",
      "Factorial of 69: 171122452428141311372468338881272839092270544893520369393648040923257279754140647424000000000000000\n",
      "Factorial of 70: 11978571669969891796072783721689098736458938142546425857555362864628009582789845319680000000000000000\n",
      "Factorial of 71: 850478588567862317521167644239926010288584608120796235886430763388588680378079017697280000000000000000\n",
      "Factorial of 72: 61234458376886086861524070385274672740778091784697328983823014963978384987221689274204160000000000000000\n",
      "Factorial of 73: 4470115461512684340891257138125051110076800700282905015819080092370422104067183317016903680000000000000000\n",
      "Factorial of 74: 330788544151938641225953028221253782145683251820934971170611926835411235700971565459250872320000000000000000\n",
      "Factorial of 75: 24809140811395398091946477116594033660926243886570122837795894512655842677572867409443815424000000000000000000\n",
      "Factorial of 76: 1885494701666050254987932260861146558230394535379329335672487982961844043495537923117729972224000000000000000000\n",
      "Factorial of 77: 145183092028285869634070784086308284983740379224208358846781574688061991349156420080065207861248000000000000000000\n",
      "Factorial of 78: 11324281178206297831457521158732046228731749579488251990048962825668835325234200766245086213177344000000000000000000\n",
      "Factorial of 79: 894618213078297528685144171539831652069808216779571907213868063227837990693501860533361810841010176000000000000000000\n",
      "Factorial of 80: 71569457046263802294811533723186532165584657342365752577109445058227039255480148842668944867280814080000000000000000000\n",
      "Factorial of 81: 5797126020747367985879734231578109105412357244731625958745865049716390179693892056256184534249745940480000000000000000000\n",
      "Factorial of 82: 475364333701284174842138206989404946643813294067993328617160934076743994734899148613007131808479167119360000000000000000000\n",
      "Factorial of 83: 39455239697206586511897471180120610571436503407643446275224357528369751562996629334879591940103770870906880000000000000000000\n",
      "Factorial of 84: 3314240134565353266999387579130131288000666286242049487118846032383059131291716864129885722968716753156177920000000000000000000\n",
      "Factorial of 85: 281710411438055027694947944226061159480056634330574206405101912752560026159795933451040286452340924018275123200000000000000000000\n",
      "Factorial of 86: 24227095383672732381765523203441259715284870552429381750838764496720162249742450276789464634901319465571660595200000000000000000000\n",
      "Factorial of 87: 2107757298379527717213600518699389595229783738061356212322972511214654115727593174080683423236414793504734471782400000000000000000000\n",
      "Factorial of 88: 185482642257398439114796845645546284380220968949399346684421580986889562184028199319100141244804501828416633516851200000000000000000000\n",
      "Factorial of 89: 16507955160908461081216919262453619309839666236496541854913520707833171034378509739399912570787600662729080382999756800000000000000000000\n",
      "Factorial of 90: 1485715964481761497309522733620825737885569961284688766942216863704985393094065876545992131370884059645617234469978112000000000000000000000\n",
      "Factorial of 91: 135200152767840296255166568759495142147586866476906677791741734597153670771559994765685283954750449427751168336768008192000000000000000000000\n",
      "Factorial of 92: 12438414054641307255475324325873553077577991715875414356840239582938137710983519518443046123837041347353107486982656753664000000000000000000000\n",
      "Factorial of 93: 1156772507081641574759205162306240436214753229576413535186142281213246807121467315215203289516844845303838996289387078090752000000000000000000000\n",
      "Factorial of 94: 108736615665674308027365285256786601004186803580182872307497374434045199869417927630229109214583415458560865651202385340530688000000000000000000000\n",
      "Factorial of 95: 10329978488239059262599702099394727095397746340117372869212250571234293987594703124871765375385424468563282236864226607350415360000000000000000000000\n",
      "Factorial of 96: 991677934870949689209571401541893801158183648651267795444376054838492222809091499987689476037000748982075094738965754305639874560000000000000000000000\n",
      "Factorial of 97: 96192759682482119853328425949563698712343813919172976158104477319333745612481875498805879175589072651261284189679678167647067832320000000000000000000000\n",
      "Factorial of 98: 9426890448883247745626185743057242473809693764078951663494238777294707070023223798882976159207729119823605850588608460429412647567360000000000000000000000\n",
      "Factorial of 99: 933262154439441526816992388562667004907159682643816214685929638952175999932299156089414639761565182862536979208272237582511852109168640000000000000000000000\n",
      "Factorial of 100: 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\n",
      "\n",
      "Fibonacci table:\n",
      "Fibonacci(0): 0\n",
      "Fibonacci(1): 1\n",
      "Fibonacci(2): 1\n",
      "Fibonacci(3): 2\n",
      "Fibonacci(4): 3\n",
      "Fibonacci(5): 5\n",
      "Fibonacci(6): 8\n",
      "Fibonacci(7): 3\n",
      "Fibonacci(8): 1\n",
      "Fibonacci(9): 4\n",
      "Fibonacci(10): 5\n",
      "Fibonacci(11): 9\n",
      "Fibonacci(12): 4\n",
      "Fibonacci(13): 3\n",
      "Fibonacci(14): 7\n",
      "Fibonacci(15): 0\n",
      "Fibonacci(16): 7\n",
      "Fibonacci(17): 7\n",
      "Fibonacci(18): 4\n",
      "Fibonacci(19): 1\n",
      "Fibonacci(20): 5\n",
      "Fibonacci(21): 6\n",
      "Fibonacci(22): 1\n",
      "Fibonacci(23): 7\n",
      "Fibonacci(24): 8\n",
      "Fibonacci(25): 5\n",
      "Fibonacci(26): 3\n",
      "Fibonacci(27): 8\n",
      "Fibonacci(28): 1\n",
      "Fibonacci(29): 9\n",
      "Fibonacci(30): 0\n",
      "Fibonacci(31): 9\n",
      "Fibonacci(32): 9\n",
      "Fibonacci(33): 8\n",
      "Fibonacci(34): 7\n",
      "Fibonacci(35): 5\n",
      "Fibonacci(36): 2\n",
      "Fibonacci(37): 7\n",
      "Fibonacci(38): 9\n",
      "Fibonacci(39): 6\n",
      "Fibonacci(40): 5\n",
      "Fibonacci(41): 1\n",
      "Fibonacci(42): 6\n",
      "Fibonacci(43): 7\n",
      "Fibonacci(44): 3\n",
      "Fibonacci(45): 0\n",
      "Fibonacci(46): 3\n",
      "Fibonacci(47): 3\n",
      "Fibonacci(48): 6\n",
      "Fibonacci(49): 9\n",
      "Fibonacci(50): 5\n",
      "Fibonacci(51): 4\n",
      "Fibonacci(52): 9\n",
      "Fibonacci(53): 3\n",
      "Fibonacci(54): 2\n",
      "Fibonacci(55): 5\n",
      "Fibonacci(56): 7\n",
      "Fibonacci(57): 2\n",
      "Fibonacci(58): 9\n",
      "Fibonacci(59): 1\n",
      "Fibonacci(60): 0\n",
      "Fibonacci(61): 1\n",
      "Fibonacci(62): 1\n",
      "Fibonacci(63): 2\n",
      "Fibonacci(64): 3\n",
      "Fibonacci(65): 5\n",
      "Fibonacci(66): 8\n",
      "Fibonacci(67): 3\n",
      "Fibonacci(68): 1\n",
      "Fibonacci(69): 4\n",
      "Fibonacci(70): 5\n",
      "Fibonacci(71): 9\n",
      "Fibonacci(72): 4\n",
      "Fibonacci(73): 3\n",
      "Fibonacci(74): 7\n",
      "Fibonacci(75): 0\n",
      "Fibonacci(76): 7\n",
      "Fibonacci(77): 7\n",
      "Fibonacci(78): 4\n",
      "Fibonacci(79): 1\n",
      "Fibonacci(80): 5\n",
      "Fibonacci(81): 6\n",
      "Fibonacci(82): 1\n",
      "Fibonacci(83): 7\n",
      "Fibonacci(84): 8\n",
      "Fibonacci(85): 5\n",
      "Fibonacci(86): 3\n",
      "Fibonacci(87): 8\n",
      "Fibonacci(88): 1\n",
      "Fibonacci(89): 9\n",
      "Fibonacci(90): 0\n",
      "Fibonacci(91): 9\n",
      "Fibonacci(92): 9\n",
      "Fibonacci(93): 8\n",
      "Fibonacci(94): 7\n",
      "Fibonacci(95): 5\n",
      "Fibonacci(96): 2\n",
      "Fibonacci(97): 7\n",
      "Fibonacci(98): 9\n",
      "Fibonacci(99): 6\n",
      "\n",
      "Integer table:\n",
      "0: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "1: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01'\n",
      "2: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02'\n",
      "3: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03'\n",
      "4: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04'\n",
      "5: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05'\n",
      "6: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06'\n",
      "7: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07'\n",
      "8: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08'\n",
      "9: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t'\n",
      "10: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n'\n",
      "11: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0b'\n",
      "12: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0c'\n",
      "13: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\r'\n",
      "14: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0e'\n",
      "15: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0f'\n",
      "16: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10'\n",
      "17: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x11'\n",
      "18: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x12'\n",
      "19: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x13'\n",
      "20: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14'\n",
      "21: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15'\n",
      "22: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16'\n",
      "23: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x17'\n",
      "24: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18'\n",
      "25: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19'\n",
      "26: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1a'\n",
      "27: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1b'\n",
      "28: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c'\n",
      "29: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1d'\n",
      "30: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1e'\n",
      "31: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1f'\n",
      "32: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00 '\n",
      "33: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00!'\n",
      "34: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"'\n",
      "35: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00#'\n",
      "36: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00$'\n",
      "37: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00%'\n",
      "38: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00&'\n",
      "39: b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\"\n",
      "40: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00('\n",
      "41: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00)'\n",
      "42: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00*'\n",
      "43: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00+'\n",
      "44: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00,'\n",
      "45: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00-'\n",
      "46: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00.'\n",
      "47: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00/'\n",
      "48: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x000'\n",
      "49: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x001'\n",
      "50: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x002'\n",
      "51: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x003'\n",
      "52: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x004'\n",
      "53: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x005'\n",
      "54: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x006'\n",
      "55: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x007'\n",
      "56: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x008'\n",
      "57: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x009'\n",
      "58: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00:'\n",
      "59: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00;'\n",
      "60: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00<'\n",
      "61: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00='\n",
      "62: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00>'\n",
      "63: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00?'\n",
      "64: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00@'\n",
      "65: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00A'\n",
      "66: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00B'\n",
      "67: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00C'\n",
      "68: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00D'\n",
      "69: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00E'\n",
      "70: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00F'\n",
      "71: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00G'\n",
      "72: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00H'\n",
      "73: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00I'\n",
      "74: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00J'\n",
      "75: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00K'\n",
      "76: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00L'\n",
      "77: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00M'\n",
      "78: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00N'\n",
      "79: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00O'\n",
      "80: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00P'\n",
      "81: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00Q'\n",
      "82: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00R'\n",
      "83: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00S'\n",
      "84: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00T'\n",
      "85: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00U'\n",
      "86: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00V'\n",
      "87: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00W'\n",
      "88: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00X'\n",
      "89: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00Y'\n",
      "90: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z'\n",
      "91: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00['\n",
      "92: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\\\'\n",
      "93: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00]'\n",
      "94: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00^'\n",
      "95: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00_'\n",
      "96: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00`'\n",
      "97: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00a'\n",
      "98: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00b'\n",
      "99: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00c'\n",
      "100: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00d'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m pattern:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start \u001b[38;5;129;01mand\u001b[39;00m num \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end:\n\u001b[1;32m---> 59\u001b[0m         mapped_tables[num] \u001b[38;5;241m=\u001b[39m (factorial_table[num \u001b[38;5;241m-\u001b[39m start], \u001b[43mfibonacci_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m]\u001b[49m, integer_table[num])\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         mapped_tables[num] \u001b[38;5;241m=\u001b[39m (factorial(num), fibonacci(num), \u001b[38;5;28mint\u001b[39m(num)\u001b[38;5;241m.\u001b[39mto_bytes(\u001b[38;5;241m8\u001b[39m, byteorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbig\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Leave the base number as a value\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_tables(start, end):\n",
    "    # Factorial function\n",
    "    def factorial(n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return n * factorial(n-1)\n",
    "    \n",
    "    # Fibonacci function\n",
    "    def fibonacci(n):\n",
    "        fibonacci_sequence = [0, 1]  # Initialize with the first two Fibonacci numbers\n",
    "        while len(fibonacci_sequence) < n:\n",
    "            next_fib = (fibonacci_sequence[-1] + fibonacci_sequence[-2]) % 10  # Calculate the next Fibonacci number modulo 10\n",
    "            fibonacci_sequence.append(next_fib)\n",
    "        return fibonacci_sequence\n",
    "    \n",
    "    # Integer table function\n",
    "    def create_integer_table(start, end):\n",
    "        integer_table = {}\n",
    "        for i in range(start, end+1):\n",
    "            byte_format = i.to_bytes(8, byteorder='big')  # Convert integer to byte format (8 bytes)\n",
    "            integer_table[i] = byte_format\n",
    "        return integer_table\n",
    "    \n",
    "    # Create tables\n",
    "    factorial_table = [factorial(i) for i in range(start, end+1)]\n",
    "    fibonacci_table = fibonacci(end)\n",
    "    integer_table = create_integer_table(start, end)\n",
    "    \n",
    "    return factorial_table, fibonacci_table, integer_table\n",
    "\n",
    "# Call the function to create the tables for the range 0 to 100\n",
    "start = 0\n",
    "end = 100\n",
    "factorial_table, fibonacci_table, integer_table = create_tables(start, end)\n",
    "\n",
    "# Print the tables\n",
    "print(\"Factorial table:\")\n",
    "for i, factorial_value in enumerate(factorial_table):\n",
    "    print(f\"Factorial of {i}: {factorial_value}\")\n",
    "\n",
    "print(\"\\nFibonacci table:\")\n",
    "for i, fib_number in enumerate(fibonacci_table):\n",
    "    print(f\"Fibonacci({i}): {fib_number}\")\n",
    "\n",
    "print(\"\\nInteger table:\")\n",
    "for integer, byte_format in integer_table.items():\n",
    "    print(f\"{integer}: {byte_format}\")\n",
    "\n",
    "# Read the pattern of numbers from a CSV file\n",
    "pattern = pd.read_csv('a.csv', header=None).values.flatten()\n",
    "\n",
    "# Create a dictionary to map the pattern numbers to table values\n",
    "mapped_tables = {}\n",
    "for num in pattern:\n",
    "    if num >= start and num <= end:\n",
    "        mapped_tables[num] = (factorial_table[num - start], fibonacci_table[num - start], integer_table[num])\n",
    "    else:\n",
    "        mapped_tables[num] = (factorial(num), fibonacci(num), int(num).to_bytes(8, byteorder='big'))  # Leave the base number as a value\n",
    "\n",
    "# Print the mapped tables\n",
    "for num, values in mapped_tables.items():\n",
    "    print(f\"Number: {num}\")\n",
    "    print(f\"Factorial: {values[0]}\")\n",
    "    print(f\"Fibonacci: {values[1]}\")\n",
    "    print(f\"Integer: {values[2]}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
